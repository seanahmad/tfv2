{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task: To predict hotel cancellations using a Keras-based neural network.\n",
    "\n",
    "## Original hotel booking demand datasets by authors Nuno Antonio, Ana de Almeida, and Luis Nunes available at:\n",
    "\n",
    "### https://www.sciencedirect.com/science/article/pii/S2352340918315191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same neural network is run, but this time using TF 2.0 standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training dataset H1 and sort by year and week number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('H1.csv')\n",
    "a=train_df.head()\n",
    "b=train_df\n",
    "b\n",
    "b.sort_values(['ArrivalDateYear','ArrivalDateWeekNumber'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent variable (y). Cancellation by customer = 1, no cancellation by customer = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IsCanceled = train_df['IsCanceled']\n",
    "y = IsCanceled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features (or independent variables) hypothesised to influence hotel cancellations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leadtime = train_df['LeadTime'] #1\n",
    "staysweekendnights = train_df['StaysInWeekendNights'] #2\n",
    "staysweeknights = train_df['StaysInWeekNights'] #3\n",
    "adults = train_df['Adults'] #4\n",
    "children = train_df['Children'] #5\n",
    "babies = train_df['Babies'] #6\n",
    "isrepeatedguest = train_df['IsRepeatedGuest'] #11\n",
    "previouscancellations = train_df['PreviousCancellations'] #12\n",
    "previousbookingsnotcanceled = train_df['PreviousBookingsNotCanceled'] #13\n",
    "bookingchanges = train_df['BookingChanges'] #16\n",
    "agent = train_df['Agent'] #18\n",
    "company = train_df['Company'] #19\n",
    "dayswaitinglist = train_df['DaysInWaitingList'] #20\n",
    "adr = train_df['ADR'] #22\n",
    "rcps = train_df['RequiredCarParkingSpaces'] #23\n",
    "totalsqr = train_df['TotalOfSpecialRequests'] #24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables - variables that do not have an interval scale, e.g. 1-100.\n",
    "\n",
    "### cat.codes is being used to define these categorical variables, as assigning a number to each variable without specifying that variable as a category will lead to Python treating each variable as interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mealcat=train_df.Meal.astype(\"category\").cat.codes\n",
    "mealcat=pd.Series(mealcat)\n",
    "countrycat=train_df.Country.astype(\"category\").cat.codes\n",
    "countrycat=pd.Series(countrycat)\n",
    "marketsegmentcat=train_df.MarketSegment.astype(\"category\").cat.codes\n",
    "marketsegmentcat=pd.Series(marketsegmentcat)\n",
    "distributionchannelcat=train_df.DistributionChannel.astype(\"category\").cat.codes\n",
    "distributionchannelcat=pd.Series(distributionchannelcat)\n",
    "reservedroomtypecat=train_df.ReservedRoomType.astype(\"category\").cat.codes\n",
    "reservedroomtypecat=pd.Series(reservedroomtypecat)\n",
    "assignedroomtypecat=train_df.AssignedRoomType.astype(\"category\").cat.codes\n",
    "assignedroomtypecat=pd.Series(assignedroomtypecat)\n",
    "deposittypecat=train_df.DepositType.astype(\"category\").cat.codes\n",
    "deposittypecat=pd.Series(deposittypecat)\n",
    "customertypecat=train_df.CustomerType.astype(\"category\").cat.codes\n",
    "customertypecat=pd.Series(customertypecat)\n",
    "reservationstatuscat=train_df.ReservationStatus.astype(\"category\").cat.codes\n",
    "reservationstatuscat=pd.Series(reservationstatuscat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minmax_scale is used to scale the relevant independent variable (in this case, lead time) to a value between 0 and 1.\n",
    "\n",
    "### If the variables in the neural network do not have a common scale, then it will increase the likelihood of incorrect interpretations by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "leadtime = minmax_scale(leadtime)\n",
    "leadtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All independent variables are stacked together using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.column_stack((leadtime,deposittypecat,countrycat))\n",
    "x1 = sm.add_constant(x1, prepend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A train-test split is used to partition the data into training (used to train the model), and validation (check predictions) against the actual cancellation incidences and update the model accordingly.\n",
    "\n",
    "### The actual test data (or unseen data) is the H2 dataset - you will use this soon to test the model predictions against the actual cancellations for H2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In TF v1.0, Sequential, Dense, and KerasRegressor were being imported separately. In TF 2.0, models and layers are being imported from keras, and the above no longer need to be manually imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'sigmoid' is used as the activation function for the output node given that this is a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_shape=(4,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The adam optimizer is used to train the model, and the binary_crossentropy is used as the loss function.\n",
    "\n",
    "### Loss = degree of error between the predicted and actual values.\n",
    "\n",
    "### 500 epochs (or forward and backward passes) are used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history=model.fit(x1_train,\n",
    "                  y1_train,\n",
    "                  epochs=500,\n",
    "                  batch_size=512,\n",
    "                  validation_data=(x1_test, y1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a plot of the train and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a plot of the train vs. validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are the predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC (or area under the curve) is used to assess the classifier accuracy.\n",
    "\n",
    "### An AUC of 0.5 means that the model performs no better than random guessing. A value above 0.5 means that the model has predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "falsepos, truepos, thresholds = roc_curve(y1_test, pred)\n",
    "\n",
    "auc = roc_auc_score(y1_test, pred)\n",
    "print('AUC: %.3f' % auc)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y1_test, pred)\n",
    "plt.plot([0, 1], [0, 1], linestyle='-')\n",
    "plt.plot(falsepos, truepos, marker='.')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## H2 - TEST SET WITH UNSEEN DATA\n",
    "\n",
    "### Now, data from H2.csv (or the second hotel in the study) is being imported and processed. As you will see in the exercise below, your task is now to train the built model on the new data, generate new classifications and compare these to the existing records in H2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2data = pd.read_csv('H2.csv')\n",
    "a=h2data.head()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_leadtime = h2data['LeadTime'] #1\n",
    "t_staysweekendnights = h2data['StaysInWeekendNights'] #2\n",
    "t_staysweeknights = h2data['StaysInWeekNights'] #3\n",
    "t_adults = h2data['Adults'] #4\n",
    "t_children = h2data['Children'] #5\n",
    "t_babies = h2data['Babies'] #6\n",
    "t_isrepeatedguest = h2data['IsRepeatedGuest'] #11\n",
    "t_previouscancellations = h2data['PreviousCancellations'] #12\n",
    "t_previousbookingsnotcanceled = h2data['PreviousBookingsNotCanceled'] #13\n",
    "t_bookingchanges = h2data['BookingChanges'] #16\n",
    "t_agent = h2data['Agent'] #18\n",
    "t_company = h2data['Company'] #19\n",
    "t_dayswaitinglist = h2data['DaysInWaitingList'] #20\n",
    "t_adr = h2data['ADR'] #22\n",
    "t_rcps = h2data['RequiredCarParkingSpaces'] #23\n",
    "t_totalsqr = h2data['TotalOfSpecialRequests'] #24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mealcat=h2data.Meal.astype(\"category\").cat.codes\n",
    "t_mealcat=pd.Series(t_mealcat)\n",
    "t_countrycat=h2data.Country.astype(\"category\").cat.codes\n",
    "t_countrycat=pd.Series(t_countrycat)\n",
    "t_marketsegmentcat=h2data.MarketSegment.astype(\"category\").cat.codes\n",
    "t_marketsegmentcat=pd.Series(t_marketsegmentcat)\n",
    "t_distributionchannelcat=h2data.DistributionChannel.astype(\"category\").cat.codes\n",
    "t_distributionchannelcat=pd.Series(t_distributionchannelcat)\n",
    "t_reservedroomtypecat=h2data.ReservedRoomType.astype(\"category\").cat.codes\n",
    "t_reservedroomtypecat=pd.Series(t_reservedroomtypecat)\n",
    "t_assignedroomtypecat=h2data.AssignedRoomType.astype(\"category\").cat.codes\n",
    "t_assignedroomtypecat=pd.Series(t_assignedroomtypecat)\n",
    "t_deposittypecat=h2data.DepositType.astype(\"category\").cat.codes\n",
    "t_deposittypecat=pd.Series(t_deposittypecat)\n",
    "t_customertypecat=h2data.CustomerType.astype(\"category\").cat.codes\n",
    "t_customertypecat=pd.Series(t_customertypecat)\n",
    "t_reservationstatuscat=h2data.ReservationStatus.astype(\"category\").cat.codes\n",
    "t_reservationstatuscat=pd.Series(t_reservationstatuscat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_leadtime=np.array(t_leadtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lttest = minmax_scale(t_leadtime)\n",
    "lttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.column_stack((lttest,t_deposittypecat,t_countrycat))\n",
    "t1 = sm.add_constant(t1, prepend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IsCanceled = h2data['IsCanceled']\n",
    "b = IsCanceled\n",
    "b=b.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# EXERCISE\n",
    "\n",
    "## Using the model that has been built, generate new predictions for H2 and generate an AUC curve plotting the true vs. false positive rate.\n",
    "\n",
    "### Hint: You must use the model.predict command on the new data you are trying to predict, and then generate the AUC curve in the same manner as before by comparing the predictions to the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
